{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv \n",
    "import xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wb = xlrd.open_workbook('bad_ones.xlsx')\n",
    "wg = xlrd.open_workbook('good_ones.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb = wb.sheet_by_name(u'Feuil1') #bad_ones in feuil 1 \n",
    "sg = wg.sheet_by_name(u'Feuil1') #good_ones in feuil 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "339\n"
     ]
    }
   ],
   "source": [
    "print(sb.nrows) #nb of rows \n",
    "print(sg.nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_com = []\n",
    "good_com = []\n",
    "bad_com = sb.col_values(0) #getting data in the first column \n",
    "good_com = sg.col_values(0) #same here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_bad_data = bad_com[:int(len(bad_com)*0.25)]\n",
    "test_good_data = good_com[:int(len(good_com)*0.25)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T3ayefffffff', 'üòñüòñüòñ\\nB', '22dt', 'w', 'kolha', '3jinaaa', 'w', 'mafeha', 'hatta', 'benna', 'w', 'hatta', 'gout\\nMayghoroukomch', 'tsawer', 'enchala', 'ma', 'to7slouch', 'kifi', '3al', '7it', '', '3al', '7it', '', 'el', '8le', 'wel', 'kwa', '', 'fama', 'des', 'questions', 'yetar7ou', 'kima', 'chbih', 'ma', 'fama', 'ken', 'croissant', 'wa7da', 'w', 'chnoua', 'elli', 'fil', 'kess', 'la7mar', '?', '', 'el', 'kathra', 'w', '9ilit', 'el', 'manf3a', '', 'el', '9onbla', 'crepe', 'sauce', '', 'glace', 't3ayeff', '', '3jebni', 'le', 'commentaire', 'üòÇ', 'masit', 'kolou', 'cholosterol', 'kifeh', 'teklou', 'hetha', 'el', 'kol', 'w', 'zid', 'b', '30', 'dt', 'to93dou', 'testann√©w', 'barsha', 'aman', '?', 'Ahmed', 'Zouaoui', 'chnwa', 'rayk', 'Mriiwa', 'Aloui', 'Maw', 'tfehemna', 'manech', 'mte3', 'pti', 'dej', ':D', 'pti', 'dej', '√†', '15h', ':D', 'hhhhhhhhhhh', 'Issam', 'Ghanem', 'nhar', 'calorie', 'fel', 'smugs', 'Mz3ejbekch', '???', 'Yhebelha', 'chhar', 'entrainement', 'ba3d', 'tachkila', 'hedhi', ':', 'D', 'trah', 'a3tini', 'bo993a', '5ir', 'w', 'ar5ess', 'pr', 'un', 'brunch', 'mme', 'mouha', 'sfar???', 'sahha', \"c'est\", 'pour', 'combien', 'de', 'personnes', '?', 'Ahahahah', 'tbarkallah', 'aala', 'wkhayy', 'chnouma', 'les', 'suprises', 'et', 'soyez', 'pr√©cise', 'svp,', 'merci', 'Classic', 'hamburger', 'not', 'that', 'bad', ';', 'better', 'to', 'avoid', 'the', 'mustard', ';', 'but', 'as', 'something', 'to', 'drink', 'i', 'really', 'recommend', 'the', 'caramel', 'macchiato', 'so', 'sweat', '!', 'mana3rach', '3leh', 'fi', 'tounes', 'el', '7aja', 'awel', 'mat7el', 'ta7founa', 'w', 'ba3ed', 'temchi', 'w', 'tetbalbes.cheesecake', 't9oul', 'men', '3am', 'el', '7arb', '??', 'Avoid', 'bubble', 'gum', 'cold', 'drink,', 'otherwise', \"it's\", 'a', '5/5', ':)', 'When', 'in', '\"30', 'minutes', \"it'll\", 'be', 'ready\"', 'turns', 'into', 'more', 'than', 'an', 'hour', 'of', 'waiting.', 'I', \"wouldn't\", 'come', 'here', 'again', 'despite', 'the', 'food', 'being', 'good', 'Kol', 'chay', 'bnine', '??', 'all', 'is', 'good', 'but', 'not', 'panini.', 'Horrible', 'service', 'yet', 'a', 'very', 'delicious', 'burger', 'Service', 'merdique...', 'Bad', 'service', '????????????', '????????????', 'J‚Äôai', 'beaucoup', 'entendu', 'parler', 'de', 'l‚Äôendroit', 'mais', 'je', 'suis', 'finalement', 'd√©√ßu.', 'Table', 'sale,', 'et', 'le', 'pain', 'utilis√©', 'dans', 'le', 'sandwich', 'est', 'de', 'mauvaise', 'qualit√©.', 'Et', 'j‚Äôai', 'm√¢ch√©', 'un', 'os', 'au', 'passage', 'dans', 'la', 'garniture.', '??', 'Sweet', 'breakfast', '???', '√†', 'ne', 'pas', 'rater', '?', 'Le', 'meilleur', 'hambuger', 'que', \"j'ai\", 'go√ªt√©,un', 'vrai', 'd√©lice', '??', 'Merci', 'aux', 'organisateurs', '.', \"C'√©tait\", 'bien', '...mais', 'bon...', 'Quand', 'on', 'a', 'pr√®s', 'de', '40', 'ans', 'et', 'veux', 'qui', 'sont', 'autour', 'ont', 'la', 'moiti√©', 'de', 'mon', '√¢ge', '...on', 'se', 'sent', 'heureux', 'de', 'voir', 'ces', 'gens', 'sourire', 'et', '√©changer', '...', 'Les', 'prix', 'par', 'rapport', '√†', 'la', 'quantit√©', 'sont', 'exag√©r√©s..', 'sinon', 'tout', 'est', 'excellent', ':-)', 'Le', 'burger', 'est', 'bon', 'mais', 'pas', 'd√©licieux.', 'Pr', 'le', 'm√™me', 'prix', 'je', 'pr√©f√®re', 'le', 'zink..', \"J'adoooore\", 'ce', 'caf√©', 'tt', 'est', 'parfait:', 'bouffe,', 'boissons,', 'personnel...', '????', 'Tr√®s', 'bonne', 'bouffe', 'malheureusement', 'le', 'service', 'est', 'trreeeeeees', 'tr√®s', 'tr√®s', 'mais', 'vraiment', 'tr√®s', 'lent!!!', 'Passer', '1h', 'pour', 'avoir', 'son', 'petit', 'dej,', 'non', 'merci', '!!!', 'Cheesecacke', 'yesser', 'bnin', 'w', 'service', 'excellent', 'mais', 'pour', 'un', 'samedi', 'yesser', 'joujma', 'Les', 'cookies', 'sont', 'merdiques!!', 'Non', 'mais', 'vraiment', 'ils', 'sont', 'pas', 'digne', \"d'√™tre\", 'appell√©s', 'des', 'cookies', '!!!', \"C'est\", 'des', 'pauvres', 'minables', \"'bachkoutou'\", 'pas', 'plus....', \"Aujourd'hui\", \"c'etait\", 'la', 'catastrophe', 'a', 'smugs', ',', 'le', 'serveur', 'nous', 'ramene', 'une', 'fausse', 'demande', 'et', 'il', 'a', 'refus√©', 'de', 'la', 'changer', '!!', \"C'est\", 'impoli', '!!!', 'Le', 'cup', 'cake', 'date', 'de', 'la', 'premi√®re', 'guerre', 'mondiale.Il', 'est', 'temps', 'de', 'boycotter', 'Service', 'y3ayyef', 'malgr√©', 'que', \"l'endroit\", 'et', 'les', 'produits', 'behin...', 'Lezem', 'houni,', 'tji', 'eamel', '7esba..', 'tatla3', '7esba..', 'tu', 'peux', 'pas', 'te', 'contr√¥ler', '*.*', 'DELISH', '?', 'Le', 'service', 'se', 'd√©grade', 'de', 'plus', 'en', 'plus', \"c'est\", 'vraiment', 'dommage', 'Rien', \"n'est\", 'plus', 'comme', 'avant', '!', 'Service', 'beaucoup', 'trop', 'long', 'avec', 'un', 'caf√©', 'totalement', 'glac√©', 'apr√®s', '1h30', \"d'attente!\", 'Aucun', 'respect', 'de', 'priorit√©!', 'Le', 'cup', 'cake', 'date', 'de', 'la', 'premi√®re', 'guerre', 'mondiale.La', 'qualit√©', 'de', 'pire', 'en', 'pire', '√†', 'chaque', 'fois.', 'Il', 'est', 'temps', 'de', 'boycotter', 'Gateau', 'au', 'chocolat', '????', 'Pas', 'trop', 'satisfaite', 'par', 'rapport', 'qualit√©', 'prix']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-65ef41067274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcluster_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mexemplar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maffprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_indices_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mcluster_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_bad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for i in train_bad_data:        #splitting phrases into words \n",
    "    words.append(i.split(\" \"))\n",
    "    \n",
    "dictio = []    \n",
    "for sublist in words:\n",
    "    for item in sublist:\n",
    "        dictio.append(item)  \n",
    "dictio = np.asarray(dictio) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words]) #calculating the similarity with lev distance\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity) \n",
    "train_bad = []\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = dictio[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(dictio[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(map(str, cluster))\n",
    "    train_bad.append(cluster)\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
