{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Facebook scrapper</h2>\n",
    "\n",
    "One of the main problematics that we faced inthe project was the unability to access Facebook's closed groupes such as \"On a mangé pour vous\" with the graph api ( Facebook's data extraction API ). To face this problem we decided to create a Facebook scrapping script.\n",
    "\n",
    "Our script uses selenium as a headless browser to access the closed groupes and extract the different posts and comments. This script was mainly inspired by <a href = \"https://github.com/hikaruAi/FacebookBot\">hikaruAi's FacebookBot</a> and adapt few methods from it such as the login and logout methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from getpass import getpass\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import time\n",
    "import os\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Facebook scrappers go through the mbasic version of the website. This version is a light Facebook that is optimised for fast browsing and low bandwidth usage, thus, it's html and javascript codes are simpler than the Desktop version which makes data extraction easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mfacebookToBasic(url):\n",
    "    \"\"\"(hikaruAi method)Reformat a url to load mbasic facebook instead of regular facebook, return the same string if\n",
    "    the url don't contains facebook\"\"\"\n",
    "\n",
    "    if \"m.facebook.com\" in url:\n",
    "        return url.replace(\"m.facebook.com\", \"mbasic.facebook.com\")\n",
    "    elif \"www.facebook.com\" in url:\n",
    "        return url.replace(\"www.facebook.com\", \"mbasic.facebook.com\")\n",
    "    else:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FacebookScrapper is the main class in this script. it extends the PhantomJS webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FacebookScrapper(webdriver.PhantomJS):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        # pathToPhantomJs =\"\n",
    "        \"\"\"(hikaruAi method) relativePhatomJs = \"\\\\phantomjs.exe\"\n",
    "        service_args = None\n",
    "        if images == False:\n",
    "            service_args = ['--load-images=no', ]\n",
    "        if pathToPhantomJs == None:\n",
    "            path = self, os.getcwd() + relativePhatomJs\n",
    "        else:\n",
    "            path = pathToPhantomJs\n",
    "            webdriver.PhantomJS.__init__(self, path, service_args=service_args)\n",
    "        \"\"\"\n",
    "        \n",
    "        dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "        dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53 \"\n",
    "            \"(KHTML, like Gecko) Chrome/15.0.87\"\n",
    "        )\n",
    "        webdriver.PhantomJS.__init__(self, executable_path=\"C:\\\\Users\\\\Ska\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\phantomjs-prebuilt\\\\lib\\\\phantom\\\\bin\\\\phantomjs.exe\",desired_capabilities=dcap)\n",
    "        \n",
    "        self.set_window_size (1000,500)\n",
    "    def get(self, url):\n",
    "        \"\"\"(hikaruAi method )The make the driver go to the url but reformat the url if is for facebook page\"\"\"\n",
    "        super().get(mfacebookToBasic(url))\n",
    "       \n",
    "\n",
    "    def login(self, email, password):\n",
    "        \"\"\"(hikaruAi method )Log to facebook using email (str) and password (str)\"\"\"\n",
    "\n",
    "        url = \"https://mbasic.facebook.com\"\n",
    "        self.get(url)\n",
    "        email_element = self.find_element_by_name(\"email\")\n",
    "        email_element.send_keys(email)\n",
    "        pass_element = self.find_element_by_name(\"pass\")\n",
    "        pass_element.send_keys(password)\n",
    "        pass_element.send_keys(Keys.ENTER)\n",
    "        if self.find_element_by_class_name(\"bi\"):\n",
    "            self.find_element_by_class_name(\"bp\").click();        \n",
    "        try:\n",
    "            self.find_element_by_name(\"xc_message\")\n",
    "            print(\"Logged in\")\n",
    "            return True\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"Fail to login : either connection problem or you are already logged in\")\n",
    "            return False\n",
    "\n",
    "    def logout(self):\n",
    "        \"\"\"Log out from Facebook\"\"\"\n",
    "\n",
    "        url = \"https://mbasic.facebook.com/logout.php?h=AffSEUYT5RsM6bkY&t=1446949608&ref_component=mbasic_footer&ref_page=%2Fwap%2Fhome.php&refid=7\"\n",
    "        try:\n",
    "            self.get(url)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Failed to log out ->\\n\", e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>getReviews:</h4> this function is mainly used to extract reviews their url and the date at which they were posted from Facebook Pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getReviews(bot,url) :\n",
    "    bot.get(url)\n",
    "    # list initialisation\n",
    "    links_list =[]\n",
    "    dates_list = []\n",
    "    posts_list = []\n",
    "    # we extract the links to each review by searching for <a> tags. Here we noticed that in every review's\n",
    "    # link contains the 'activity' string, therefore, we check if the link contains this string.\n",
    "    for a in bot.find_elements_by_tag_name('a'):\n",
    "        if 'activity' in a.get_attribute('href'):\n",
    "            links_list.append (a.get_attribute('href'))\n",
    "\n",
    "    links_list=list ( set(links_list) )\n",
    "    \n",
    "    for l in links_list :\n",
    "        # we go into every extracted link\n",
    "        bot.get(l)\n",
    "        try : \n",
    "            # a review's text is the only one inside the <p>\n",
    "            review = bot.find_element_by_tag_name('p').text\n",
    "            #the review's date is contained in the first <abbr> tag\n",
    "            date = bot.find_elements_by_tag_name('abbr')[0].text\n",
    "            posts_list.append(review)\n",
    "            dates_list.append(date)\n",
    "\n",
    "        except Exception as e:\n",
    "            #in case of failure we append an empty string to keep matching indexes between the links_list and the otehr 2 lists \n",
    "            posts_list.append('')\n",
    "            dates_list.append('')\n",
    "\n",
    "    return posts_list, links_list, dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>GetPostInGroup:</h4> The function that extracts post from Facebook groups the urls to their comments and their posting date. This function is a deeply modified and more efficient version of HikaruAi's and unlike the older version it succeeds at extracting all posts. Given a URL and a depth it searchs a pages and extracts all the post. The moreText argument allows it to find the link to the next batch of posts note that <strong>this String changes along your facebook's language setting</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPostInGroup(bot, url, deep=10, moreText=\"Show more\"):\n",
    "        \n",
    "        # we go into the group's url\n",
    "        bot.get(url)\n",
    "        posts = []\n",
    "        comments_links = []\n",
    "        dates=[]\n",
    "        for n in range(deep):\n",
    "            # the main loop of post extraction, it goes through the posts following until it reaches depth deep\n",
    "            print(\"Searching, deep \",n)\n",
    "            # every post's id in a group has the 'u_0_' string in it, for this reason we use the xpath to extract them\n",
    "            posts_element_list = bot.find_elements_by_xpath(\"//*[contains(@id,'u_0_')]\")\n",
    "            \n",
    "            for p in posts_element_list :\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    text= p.find_element_by_tag_name(\"p\").text\n",
    "                    #we extract the link to the comments by looking for the \"Comment\" substring in the html code \n",
    "                    linkToComment = p.find_element_by_partial_link_text('Comment').get_attribute('href')\n",
    "                    date = p.find_element_by_tag_name(\"abbr\").text\n",
    "\n",
    "                    if text not in posts:\n",
    "                        # the use of the \"//*[contains(@id,'u_0_')]\" extracts every link twice. and since we didn't find\n",
    "                        # a more efficient trick to extract every single comment we decided to just check if the post\n",
    "                        # was already extracted\n",
    "                        posts.append(text)\n",
    "                        comments_links.append(linkToComment)\n",
    "                        dates.append(date)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                # once this packet extraction is done we go into the next depth by going into the moreText's url\n",
    "                more = bot.find_element_by_partial_link_text(\n",
    "                    moreText).get_attribute('href')\n",
    "                bot.get(more)\n",
    "            except Exception as e:\n",
    "                print(\" Can't get more posts\")\n",
    "                break\n",
    "        print ( posts)\n",
    "        return posts,comments_links,dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>getCommentsInPost:</h4> This function is quite similar to getPostInGroup. Here we focus on extracting all the comments and their date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCommentsInPost(bot, url, deep=10, moreText=\"View previous comments…\"):\n",
    "\n",
    "        bot.get(url)    \n",
    "        comments = []\n",
    "        dates = []\n",
    "        for n in range(deep):\n",
    "            if n >=0 :\n",
    "                print(\"Searching, deep \",n)\n",
    "                # To extract comments we first look for the <h3> tags that envelop the commenter's name.\n",
    "                h_list = bot.find_elements_by_tag_name('h3')\n",
    "                for h in h_list :\n",
    "                    try :\n",
    "                        # the element containing the comment is the either first or second sibling of the commenter's \n",
    "                        # name element for this reason we go through the following code twice and add a try statement\n",
    "                        c = h.find_elements_by_xpath('following-sibling::div[1]')\n",
    "                        for comment in c :\n",
    "                            try: \n",
    "                                date = h.find_element_by_xpath(\"..\").find_element_by_tag_name(\"abbr\").text\n",
    "                                comments.append(comment.text)\n",
    "                                dates.append ( date )\n",
    "                                print ( comment.text )\n",
    "                            except Exception :\n",
    "                                continue\n",
    "\n",
    "                        c = h.find_elements_by_xpath('following-sibling::div[2]')\n",
    "                        for comment in c :\n",
    "                            try:\n",
    "                                date = h.find_element_by_xpath(\"..\").find_element_by_tag_name(\"abbr\").text\n",
    "                                comments.append(comment.text)\n",
    "                                dates.append(date)\n",
    "                                print ( comment.text )   \n",
    "                            except Exception :\n",
    "                                continue    \n",
    "                    except Exception :\n",
    "                        continue                  \n",
    "            try:\n",
    "                more = bot.find_element_by_partial_link_text(\n",
    "                    moreText).get_attribute('href')\n",
    "                self.get(more)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(\" Can't get more comments\")\n",
    "                break\n",
    "        return comments,dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>extractPostandComments</h4> Here we create the output file, extract the posts then the comments and finaly write all the needed data in the outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractPostandComments(bot,url,filename):\n",
    "    # outputfile creation\n",
    "    output_file = open (filename,'w',newline='',\n",
    "                        encoding='utf-8')\n",
    "    output = csv.writer(output_file, delimiter=',',quotechar='\"')      \n",
    "\n",
    "    # post extraction\n",
    "    posts,comments_links,post_dates = getPostInGroup(bot,deep=20,url=url)  \n",
    "    for i in range(len(posts)):\n",
    "        print ( \"post \", i , \" out of \" , len(posts))\n",
    "        output.writerow([posts[i],'post',comments_links[i],post_dates[i] ] )\n",
    "        #comment extraction\n",
    "        comments,comments_dates = getCommentsInPost(bot,deep=8,url=comments_links[i] )\n",
    "        for j in range ( len(comments)):\n",
    "                output.writerow([comments[j],'comment',comments_links[i],comments_dates[j] ] )\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot = FacebookScrapper()\n",
    "    try:\n",
    "        bot.login(input(\"email : \"), getpass(\"password : \"))\n",
    "    except Exception:\n",
    "        print( 'already logged ' )\n",
    "    extractPostandComments(bot,url =\"https://mbasic.facebook.com/smugslamarsa/\",filename='smugsmarsa.csv')  \n",
    "    extractPostandComments(bot,url =\"https://mbasic.facebook.com/SmugsCoffeeShop/\",filename='smugs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
